{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e72b306e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0067fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\2021\\2학기 수업\\CV\\SR\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a73859dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data folder 존재하는 경로\n",
    "os.chdir(\"D:/2021/2학기 수업/CV/SR/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e128cfc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\2021\\2학기 수업\\CV\\SR\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db75a494",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_agu_img = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4875a2b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 1, 3, 3)\n",
      "torch.Size([8, 1, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "ab_list = []\n",
    "\n",
    "a = np.array([[[[1, 2, 3],\n",
    "                [2, 3, 4],\n",
    "                [3, 4, 5]]],\n",
    "              [[[1, 3, 4],\n",
    "                [2, 3, 4],\n",
    "                [3, 4, 5]]],\n",
    "              [[[1, 2, 3],\n",
    "                [2, 3, 4],\n",
    "                [4, 5, 6]]],\n",
    "              [[[1, 2, 3],\n",
    "                [2, 3, 4],\n",
    "                [3, 4, 5]]]])\n",
    "print(a.shape)\n",
    "\n",
    "b = np.array([[[[1, 2, 3],\n",
    "                [2, 3, 4],\n",
    "                [3, 4, 5]]],\n",
    "              [[[1, 3, 4],\n",
    "                [2, 3, 4],\n",
    "                [3, 4, 5]]],\n",
    "              [[[1, 2, 3],\n",
    "                [2, 3, 4],\n",
    "                [4, 5, 6]]],\n",
    "              [[[1, 2, 3],\n",
    "                [2, 3, 4],\n",
    "                [3, 4, 5]]]])\n",
    "\n",
    "\n",
    "ab_list.append(a)\n",
    "ab_list.append(b)\n",
    "ab_list=torch.FloatTensor(np.concatenate(ab_list))\n",
    "print(ab_list.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cf727317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data/bsd200\\\\104022.png', 4] ['data/yang91\\\\t28.bmp', 220]\n",
      "291 200 91\n"
     ]
    }
   ],
   "source": [
    "BSD_DATA_DIR = 'data/bsd200'\n",
    "YANG_DATA_DIR = 'data/yang91'\n",
    "\n",
    "train_list = []\n",
    "\n",
    "bsd_cnt = 0; yang_cnt = 0\n",
    "\n",
    "for i, file in enumerate(sorted(glob(BSD_DATA_DIR + '/*'))):\n",
    "    train_list.append([file, len(train_list)])\n",
    "bsd_data_size = len(train_list)\n",
    "    \n",
    "for i, file in enumerate(sorted(glob(YANG_DATA_DIR + '/*'))):\n",
    "    train_list.append([file, len(train_list)])\n",
    "yang_data_size = len(train_list) - bsd_data_size\n",
    "    \n",
    "print(train_list[4], train_list[220])\n",
    "print(len(train_list), bsd_data_size, yang_data_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c0f6491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2328\n"
     ]
    }
   ],
   "source": [
    "AGU_BSD_DATA_DIR = 'augmented_data/bsd200/'\n",
    "\n",
    "for i in range(bsd_data_size):\n",
    "    \n",
    "    if save_agu_img == True:        \n",
    "        img = Image.open(bsd_org_list[i][0])\n",
    "        flip_ud = img.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "        flip_lr = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        flip_ud_lr = flip_ud.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        rot_90 = img.transpose(Image.ROTATE_90)\n",
    "        rot_270 = img.transpose(Image.ROTATE_270)\n",
    "        rot_90_ud = rot_90.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "        rot_270_ud = rot_270.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "    \n",
    "    \n",
    "        flip_ud.save(AGU_BSD_DATA_DIR+'UD/'+str(i)+'.png')\n",
    "        flip_lr.save(AGU_BSD_DATA_DIR+'LR/'+str(i)+'.png')\n",
    "        flip_ud_lr.save(AGU_BSD_DATA_DIR+'UDLR/'+str(i)+'.png')\n",
    "        rot_90.save(AGU_BSD_DATA_DIR+'90/'+str(i)+'.png')\n",
    "        rot_270.save(AGU_BSD_DATA_DIR+'270/'+str(i)+'.png')\n",
    "        rot_90_ud.save(AGU_BSD_DATA_DIR+'90UD/'+str(i)+'.png')\n",
    "        rot_270_ud.save(AGU_BSD_DATA_DIR+'270UD/'+str(i)+'.png')\n",
    "    \n",
    "    train_list.append([AGU_BSD_DATA_DIR+'UD/'+str(i)+'.png', len(train_list)])\n",
    "    train_list.append([AGU_BSD_DATA_DIR+'LR/'+str(i)+'.png', len(train_list)])\n",
    "    train_list.append([AGU_BSD_DATA_DIR+'UDLR/'+str(i)+'.png', len(train_list)])\n",
    "    train_list.append([AGU_BSD_DATA_DIR+'90/'+str(i)+'.png', len(train_list)])\n",
    "    train_list.append([AGU_BSD_DATA_DIR+'270/'+str(i)+'.png', len(train_list)])\n",
    "    train_list.append([AGU_BSD_DATA_DIR+'90UD/'+str(i)+'.png', len(train_list)])\n",
    "    train_list.append([AGU_BSD_DATA_DIR+'270UD/'+str(i)+'.png', len(train_list)])\n",
    "    \n",
    "\n",
    "AGU_YANG_DATA_DIR = 'augmented_data/yang91/'\n",
    "\n",
    "for i in range(yang_data_size):\n",
    "    \n",
    "    if save_agu_img == True:\n",
    "        img = Image.open(yang_org_list[i][0])\n",
    "        flip_ud = img.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "        flip_lr = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        flip_ud_lr = flip_ud.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        rot_90 = img.transpose(Image.ROTATE_90)\n",
    "        rot_270 = img.transpose(Image.ROTATE_270)\n",
    "        rot_90_ud = rot_90.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "        rot_270_ud = rot_270.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "    \n",
    "    \n",
    "        flip_ud.save(AGU_YANG_DATA_DIR+'UD/'+str(i)+'.bmp')\n",
    "        flip_lr.save(AGU_YANG_DATA_DIR+'LR/'+str(i)+'.bmp')\n",
    "        flip_ud_lr.save(AGU_YANG_DATA_DIR+'UDLR/'+str(i)+'.bmp')\n",
    "        rot_90.save(AGU_YANG_DATA_DIR+'90/'+str(i)+'.bmp')\n",
    "        rot_270.save(AGU_YANG_DATA_DIR+'270/'+str(i)+'.bmp')\n",
    "        rot_90_ud.save(AGU_YANG_DATA_DIR+'90UD/'+str(i)+'.bmp')\n",
    "        rot_270_ud.save(AGU_YANG_DATA_DIR+'270UD/'+str(i)+'.bmp')\n",
    "    \n",
    "    train_list.append([AGU_YANG_DATA_DIR+'UD/'+str(i)+'.bmp', len(train_list)])\n",
    "    train_list.append([AGU_YANG_DATA_DIR+'LR/'+str(i)+'.bmp', len(train_list)])\n",
    "    train_list.append([AGU_YANG_DATA_DIR+'UDLR/'+str(i)+'.bmp', len(train_list)])\n",
    "    train_list.append([AGU_YANG_DATA_DIR+'90/'+str(i)+'.bmp', len(train_list)])\n",
    "    train_list.append([AGU_YANG_DATA_DIR+'270/'+str(i)+'.bmp', len(train_list)])\n",
    "    train_list.append([AGU_YANG_DATA_DIR+'90UD/'+str(i)+'.bmp', len(train_list)])\n",
    "    train_list.append([AGU_YANG_DATA_DIR+'270UD/'+str(i)+'.bmp', len(train_list)])\n",
    "    \n",
    "print(len(train_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "99eed49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb2ybcr_mat = np.array(\n",
    "    [[65.738 / 256.0, 129.057 / 256.0, 25.064 / 256.0],\n",
    "     [- 37.945 / 256.0, - 74.494 / 256.0, 112.439 / 256.0],\n",
    "     [112.439 / 256.0, - 94.154 / 256.0, - 18.285 / 256.0]])\n",
    "\n",
    "rgb2y_mat = np.array([[65.481 / 256.0, 128.553 / 256.0, 24.966 / 256.0]])\n",
    "\n",
    "ycbcr2rgb_mat = np.array(\n",
    "    [[298.082 / 256.0, 0, 408.583 / 256.0],\n",
    "     [298.082 / 256.0, -100.291 / 256.0, -208.120 / 256.0],\n",
    "     [298.082 / 256.0, 516.412 / 256.0, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "42996b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bicubicResize (img, scale):\n",
    "    width, height = img.shape[1], img.shape[0]\n",
    "    new_width = int(width * scale)\n",
    "    new_height = int(height * scale)\n",
    "    \n",
    "    img = Image.fromarray(img.reshape(height, width))\n",
    "    img = img.resize([new_width, new_height], resample = Image.BICUBIC)\n",
    "    img = np.asarray(img)\n",
    "    img = img.reshape(new_height, new_width, 1)\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e57af139",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getImgPatchArr(img, patch_size, stride, enable_duplicate = True):\n",
    "    img = img.reshape(img.shape[0], img.shape[1])\n",
    "    patch_size = int(patch_size)\n",
    "    stride = int(stride)\n",
    "    height, width = img.shape\n",
    "    \n",
    "    num_patch_h = 1 + (height - patch_size) // stride\n",
    "    num_patch_w = 1 + (width - patch_size) // stride\n",
    "    \n",
    "    shape = (num_patch_h, num_patch_w, patch_size, patch_size)\n",
    "    strides = (img.itemsize) * np.array([width * stride, stride, width, 1])\n",
    "    patch_arr = np.lib.stride_tricks.as_strided(img, shape = shape, strides = strides)\n",
    "    patch_arr = patch_arr.reshape(patch_arr.shape[0] * patch_arr.shape[1], 1, patch_size, patch_size)\n",
    "    \n",
    "    if enable_duplicate:\n",
    "        extra_patches = []\n",
    "        \n",
    "        # height 기준 맨 마지막 patch들 추가\n",
    "        if (height - patch_size) % stride != 0:\n",
    "            for x in range(0, width - patch_size, stride):\n",
    "                extra_patches.append(img[height - patch_size - 1 : height - 1, x : x + patch_size])\n",
    "            \n",
    "        # width 기준 맨 마지막 patch들 추가\n",
    "        if (width - patch_size) % stride != 0:\n",
    "            for y in range(0, height - patch_size, stride):\n",
    "                extra_patches.append(img[y : y + patch_size, width - patch_size - 1 : width - 1])\n",
    "                \n",
    "        if len(extra_patches) > 0:\n",
    "            org_num_patches = patch_arr.shape[0]\n",
    "            patch_arr = np.resize(patch_arr, \n",
    "                                  [org_num_patches + len(extra_patches), patch_arr.shape[1], patch_arr.shape[2], patch_arr.shape[3]])\n",
    "            \n",
    "            for i in range(len(extra_patches)):\n",
    "                extra_patches[i] = extra_patches[i].reshape([1, patch_size, patch_size])\n",
    "                patch_arr[org_num_patches + i] = extra_patches[i]\n",
    "            \n",
    "    return patch_arr           \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "73104bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 32\n",
    "scale = 2\n",
    "stride = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9bc688a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([236768, 1, 64, 64]) torch.Size([236768, 1, 32, 32]) torch.Size([236768, 1, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "hr_list = []\n",
    "lr_list = []\n",
    "bi_list = []\n",
    "\n",
    "#count = 0\n",
    "for idx in range(len(train_list)):\n",
    "    gt_img = imageio.imread(train_list[idx][0])\n",
    "    \n",
    "    gt_width, gt_height = gt_img.shape[1], gt_img.shape[0]\n",
    "    width = (gt_width // scale) * scale\n",
    "    height = (gt_height // scale) * scale\n",
    "    \n",
    "    # scale 로 나누어 떨어지지 않는 경우 나누어 떨어지도록 resize\n",
    "    if gt_width != width or gt_height != height:\n",
    "        gt_img = gt_img[:height, :width, :]\n",
    "    \n",
    "    # color 채널 수 3개로\n",
    "    num_gt_channel = gt_img.shape[2]\n",
    "    if len(gt_img.shape) >= 3 and num_gt_channel > 3:\n",
    "        gt_img = gt_img[:, :, :3]\n",
    "    \n",
    "    gt_img_y = gt_img.dot(rgb2y_mat.T) + (16.0 * 255 / 256.0)\n",
    "    lr_img_y = bicubicResize(gt_img_y, 1 / scale)\n",
    "    bi_img_y = bicubicResize(lr_img_y, scale)\n",
    "    \n",
    "    gt_y_patches = getImgPatchArr(gt_img_y, patch_size * scale, patch_size)\n",
    "    lr_y_patches = getImgPatchArr(lr_img_y, patch_size, patch_size / scale)\n",
    "    bi_y_patches = getImgPatchArr(bi_img_y, patch_size * scale, patch_size)\n",
    "    \n",
    "    hr_list.append(gt_y_patches)\n",
    "    lr_list.append(lr_y_patches)\n",
    "    bi_list.append(bi_y_patches)\n",
    "    \n",
    "    #if count == 10:\n",
    "        #print(gt_y_patches.shape, lr_y_patches.shape, bi_y_patches.shape)\n",
    "        #break\n",
    "    #count += 1\n",
    "    \n",
    "hr_list = torch.FloatTensor(np.concatenate(hr_list))\n",
    "lr_list = torch.FloatTensor(np.concatenate(lr_list))\n",
    "bi_list = torch.FloatTensor(np.concatenate(bi_list))\n",
    "\n",
    "print(hr_list.shape, lr_list.shape, bi_list.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cce3d06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "lr = 0.0001\n",
    "train_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "15177ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torch.utils.data.TensorDataset(lr_list, hr_list, bi_list)\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset, batch_size = batch_size, shuffle = False, drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "deb1116f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCSCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DCSCN, self).__init__()\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(p = 0.8)\n",
    "        self.prelu = torch.nn.PReLU()\n",
    "        \n",
    "        # Feature Extraction Level\n",
    "        self.conv1 = torch.nn.Conv2d(1, 196, 3, padding = 1, padding_mode = \"replicate\")\n",
    "        self.conv2 = torch.nn.Conv2d(196, 166, 3, padding = 1, padding_mode = \"replicate\")\n",
    "        self.conv3 = torch.nn.Conv2d(166, 148, 3, padding = 1, padding_mode = \"replicate\")\n",
    "        self.conv4 = torch.nn.Conv2d(148, 133, 3, padding = 1, padding_mode = \"replicate\")\n",
    "        self.conv5 = torch.nn.Conv2d(133, 120, 3, padding = 1, padding_mode = \"replicate\")\n",
    "        self.conv6 = torch.nn.Conv2d(120, 108, 3, padding = 1, padding_mode = \"replicate\")\n",
    "        self.conv7 = torch.nn.Conv2d(108, 97, 3, padding = 1, padding_mode = \"replicate\")\n",
    "        self.conv8 = torch.nn.Conv2d(97, 86, 3, padding = 1, padding_mode = \"replicate\")\n",
    "        self.conv9 = torch.nn.Conv2d(86, 76, 3, padding = 1, padding_mode = \"replicate\")\n",
    "        self.conv10 = torch.nn.Conv2d(76, 66, 3, padding = 1, padding_mode = \"replicate\")\n",
    "        self.conv11 = torch.nn.Conv2d(66, 57, 3, padding = 1, padding_mode = \"replicate\")\n",
    "        self.conv12 = torch.nn.Conv2d(57, 48, 3, padding = 1, padding_mode = \"replicate\")\n",
    "        \n",
    "        torch.nn.init.kaiming_normal_(self.conv1.weight)\n",
    "        torch.nn.init.kaiming_normal_(self.conv2.weight)\n",
    "        torch.nn.init.kaiming_normal_(self.conv3.weight)\n",
    "        torch.nn.init.kaiming_normal_(self.conv4.weight)\n",
    "        torch.nn.init.kaiming_normal_(self.conv5.weight)\n",
    "        torch.nn.init.kaiming_normal_(self.conv6.weight)\n",
    "        torch.nn.init.kaiming_normal_(self.conv7.weight)\n",
    "        torch.nn.init.kaiming_normal_(self.conv8.weight)\n",
    "        torch.nn.init.kaiming_normal_(self.conv9.weight)\n",
    "        torch.nn.init.kaiming_normal_(self.conv10.weight)\n",
    "        torch.nn.init.kaiming_normal_(self.conv11.weight)\n",
    "        torch.nn.init.kaiming_normal_(self.conv12.weight)\n",
    "        \n",
    "        # Reconstruction Network Level\n",
    "        #in_channel: 196+166+148+133+120+108+97+86+76+66+57+48 = 1301\n",
    "        self.A1 = torch.nn.Conv2d(1301, 64, 1)\n",
    "        self.B1 = torch.nn.Conv2d(1301, 32, 1)\n",
    "        self.B2 = torch.nn.Conv2d(32, 32, 3, padding = 1, padding_mode = \"replicate\")\n",
    "        \n",
    "        torch.nn.init.kaiming_normal_(self.A1.weight)\n",
    "        torch.nn.init.kaiming_normal_(self.B1.weight)\n",
    "        torch.nn.init.kaiming_normal_(self.B2.weight)\n",
    "        \n",
    "        # Up-sampling Network\n",
    "        self.upconv = torch.nn.Conv2d(96, 384, 3, padding = 1, padding_mode = \"replicate\")\n",
    "        self.pixel_shuffler = torch.nn.PixelShuffle(2)\n",
    "        \n",
    "        torch.nn.init.kaiming_normal_(self.upconv.weight)\n",
    "        \n",
    "        self.reconv = torch.nn.Conv2d(96, 1, 3, padding = 1, padding_mode = \"replicate\", bias = False)\n",
    "        \n",
    "        torch.nn.init.kaiming_normal_(self.reconv.weight)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x1 = self.dropout(self.prelu(self.conv1(x)))\n",
    "        x2 = self.dropout(self.prelu(self.conv2(x1)))\n",
    "        x3 = self.dropout(self.prelu(self.conv3(x2)))\n",
    "        x4 = self.dropout(self.prelu(self.conv4(x3)))\n",
    "        x5 = self.dropout(self.prelu(self.conv5(x4)))\n",
    "        x6 = self.dropout(self.prelu(self.conv6(x5)))\n",
    "        x7 = self.dropout(self.prelu(self.conv7(x6)))\n",
    "        x8 = self.dropout(self.prelu(self.conv8(x7)))\n",
    "        x9 = self.dropout(self.prelu(self.conv9(x8)))\n",
    "        x10 = self.dropout(self.prelu(self.conv10(x9)))\n",
    "        x11 = self.dropout(self.prelu(self.conv11(x10)))\n",
    "        x12 = self.dropout(self.prelu(self.conv12(x11)))\n",
    "        \n",
    "        feature_out = torch.cat([x1,x2,x3,x4,x5,x6,x7,x8,x9,x10,x11,x12], dim=1)\n",
    "        \n",
    "        a1_out = self.dropout(self.prelu(self.A1(feature_out)))\n",
    "        b1_out = self.dropout(self.prelu(self.B1(feature_out)))\n",
    "        b2_out = self.dropout(self.prelu(self.B2(b1_out)))\n",
    "        \n",
    "        recon_out = torch.cat([a1_out, b2_out], dim=1)\n",
    "        \n",
    "        pixel_shuffle_out = self.pixel_shuffler(self.upconv(recon_out))\n",
    "        out = self.reconv(pixel_shuffle_out)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fb3ed278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "87c6aa62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11838\n"
     ]
    }
   ],
   "source": [
    "total_batch_num = len(train_loader)\n",
    "print(total_batch_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3c5ba55c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCSCN(\n",
      "  (dropout): Dropout(p=0.8, inplace=False)\n",
      "  (prelu): PReLU(num_parameters=1)\n",
      "  (conv1): Conv2d(1, 196, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv2): Conv2d(196, 166, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv3): Conv2d(166, 148, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv4): Conv2d(148, 133, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv5): Conv2d(133, 120, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv6): Conv2d(120, 108, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv7): Conv2d(108, 97, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv8): Conv2d(97, 86, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv9): Conv2d(86, 76, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv10): Conv2d(76, 66, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv11): Conv2d(66, 57, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv12): Conv2d(57, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (A1): Conv2d(1301, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (B1): Conv2d(1301, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (B2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (upconv): Conv2d(96, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (pixel_shuffler): PixelShuffle(upscale_factor=2)\n",
      "  (reconv): Conv2d(96, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = DCSCN().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7cadc9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "loss_fnc = torch.nn.MSELoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7945dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/11838 Training...batch_loss: 6491804672.0\n",
      "2000/11838 Training...batch_loss: 3300675072.0\n",
      "3000/11838 Training...batch_loss: 2213294336.0\n",
      "4000/11838 Training...batch_loss: 1664895232.0\n",
      "5000/11838 Training...batch_loss: 1334280448.0\n",
      "6000/11838 Training...batch_loss: 1113080064.0\n",
      "7000/11838 Training...batch_loss: 954767744.0\n",
      "8000/11838 Training...batch_loss: 835687680.0\n",
      "9000/11838 Training...batch_loss: 742952192.0\n",
      "10000/11838 Training...batch_loss: 668728512.0\n",
      "11000/11838 Training...batch_loss: 607972480.0\n",
      "Epoch: 0/100 loss: 564938176.0\n",
      "save model epoch: 0\n",
      "1000/11838 Training...batch_loss: 89817.21875\n",
      "2000/11838 Training...batch_loss: 65897.3671875\n",
      "3000/11838 Training...batch_loss: 52355.88671875\n",
      "4000/11838 Training...batch_loss: 42838.72265625\n",
      "5000/11838 Training...batch_loss: 36087.23828125\n",
      "6000/11838 Training...batch_loss: 31464.2109375\n",
      "7000/11838 Training...batch_loss: 28207.056640625\n",
      "8000/11838 Training...batch_loss: 24960.265625\n",
      "9000/11838 Training...batch_loss: 22646.5390625\n",
      "10000/11838 Training...batch_loss: 20550.28515625\n",
      "11000/11838 Training...batch_loss: 18927.251953125\n",
      "Epoch: 1/100 loss: 17607.79296875\n",
      "1000/11838 Training...batch_loss: 241.5518798828125\n",
      "2000/11838 Training...batch_loss: 227.9231414794922\n",
      "3000/11838 Training...batch_loss: 246.92308044433594\n",
      "4000/11838 Training...batch_loss: 249.73495483398438\n",
      "5000/11838 Training...batch_loss: 214.50381469726562\n",
      "6000/11838 Training...batch_loss: 192.0242919921875\n",
      "7000/11838 Training...batch_loss: 173.0429229736328\n",
      "8000/11838 Training...batch_loss: 153.390625\n",
      "9000/11838 Training...batch_loss: 140.43380737304688\n",
      "10000/11838 Training...batch_loss: 128.11395263671875\n",
      "11000/11838 Training...batch_loss: 118.95646667480469\n",
      "Epoch: 2/100 loss: 110.81243896484375\n",
      "1000/11838 Training...batch_loss: 6.587129592895508\n",
      "2000/11838 Training...batch_loss: 5.912868022918701\n",
      "3000/11838 Training...batch_loss: 7.085971832275391\n",
      "4000/11838 Training...batch_loss: 6.724816799163818\n",
      "5000/11838 Training...batch_loss: 6.285849571228027\n",
      "6000/11838 Training...batch_loss: 6.470945835113525\n",
      "7000/11838 Training...batch_loss: 6.3143134117126465\n",
      "8000/11838 Training...batch_loss: 6.084780693054199\n",
      "9000/11838 Training...batch_loss: 5.972093105316162\n",
      "10000/11838 Training...batch_loss: 5.874456405639648\n",
      "11000/11838 Training...batch_loss: 5.774862289428711\n",
      "Epoch: 3/100 loss: 5.612675666809082\n",
      "1000/11838 Training...batch_loss: 4.629790306091309\n",
      "2000/11838 Training...batch_loss: 4.088325500488281\n",
      "3000/11838 Training...batch_loss: 4.16452693939209\n",
      "4000/11838 Training...batch_loss: 4.32642936706543\n",
      "5000/11838 Training...batch_loss: 4.114360809326172\n",
      "6000/11838 Training...batch_loss: 4.211699485778809\n",
      "7000/11838 Training...batch_loss: 4.237652778625488\n",
      "8000/11838 Training...batch_loss: 4.188851833343506\n",
      "9000/11838 Training...batch_loss: 4.185871601104736\n",
      "10000/11838 Training...batch_loss: 4.1884284019470215\n",
      "11000/11838 Training...batch_loss: 4.115326404571533\n",
      "Epoch: 4/100 loss: 4.037266731262207\n",
      "1000/11838 Training...batch_loss: 3.8443593978881836\n",
      "2000/11838 Training...batch_loss: 3.3833723068237305\n",
      "3000/11838 Training...batch_loss: 3.4536945819854736\n",
      "4000/11838 Training...batch_loss: 3.6266028881073\n",
      "5000/11838 Training...batch_loss: 3.471860885620117\n",
      "6000/11838 Training...batch_loss: 3.598897695541382\n",
      "7000/11838 Training...batch_loss: 3.6660079956054688\n",
      "8000/11838 Training...batch_loss: 3.6606063842773438\n",
      "9000/11838 Training...batch_loss: 3.686739206314087\n",
      "10000/11838 Training...batch_loss: 3.711590051651001\n",
      "11000/11838 Training...batch_loss: 3.6646177768707275\n",
      "Epoch: 5/100 loss: 3.607764720916748\n",
      "1000/11838 Training...batch_loss: 3.6532366275787354\n",
      "2000/11838 Training...batch_loss: 3.2237296104431152\n",
      "3000/11838 Training...batch_loss: 3.3037238121032715\n",
      "4000/11838 Training...batch_loss: 3.473142385482788\n",
      "5000/11838 Training...batch_loss: 3.327923059463501\n",
      "6000/11838 Training...batch_loss: 3.4388999938964844\n",
      "7000/11838 Training...batch_loss: 3.512192964553833\n",
      "8000/11838 Training...batch_loss: 3.5149435997009277\n",
      "9000/11838 Training...batch_loss: 3.545743942260742\n",
      "10000/11838 Training...batch_loss: 3.5746452808380127\n",
      "11000/11838 Training...batch_loss: 3.5327346324920654\n",
      "Epoch: 6/100 loss: 3.4808244705200195\n",
      "1000/11838 Training...batch_loss: 3.5767102241516113\n",
      "2000/11838 Training...batch_loss: 3.1580026149749756\n",
      "3000/11838 Training...batch_loss: 3.2411162853240967\n",
      "4000/11838 Training...batch_loss: 3.407440662384033\n",
      "5000/11838 Training...batch_loss: 3.2652950286865234\n",
      "6000/11838 Training...batch_loss: 3.3701045513153076\n",
      "7000/11838 Training...batch_loss: 3.4446423053741455\n",
      "8000/11838 Training...batch_loss: 3.450392007827759\n",
      "9000/11838 Training...batch_loss: 3.4829139709472656\n",
      "10000/11838 Training...batch_loss: 3.5133416652679443\n",
      "11000/11838 Training...batch_loss: 3.473395586013794\n",
      "Epoch: 7/100 loss: 3.423443078994751\n",
      "1000/11838 Training...batch_loss: 3.536494731903076\n",
      "2000/11838 Training...batch_loss: 3.12396502494812\n",
      "3000/11838 Training...batch_loss: 3.2087063789367676\n",
      "4000/11838 Training...batch_loss: 3.373077869415283\n",
      "5000/11838 Training...batch_loss: 3.2322373390197754\n",
      "6000/11838 Training...batch_loss: 3.3336310386657715\n",
      "7000/11838 Training...batch_loss: 3.408677101135254\n",
      "8000/11838 Training...batch_loss: 3.4157562255859375\n",
      "9000/11838 Training...batch_loss: 3.448884963989258\n",
      "10000/11838 Training...batch_loss: 3.479844808578491\n",
      "11000/11838 Training...batch_loss: 3.4407477378845215\n",
      "Epoch: 8/100 loss: 3.391850233078003\n",
      "1000/11838 Training...batch_loss: 3.510793447494507\n",
      "2000/11838 Training...batch_loss: 3.10245418548584\n",
      "3000/11838 Training...batch_loss: 3.1883859634399414\n",
      "4000/11838 Training...batch_loss: 3.35099720954895\n",
      "5000/11838 Training...batch_loss: 3.2110707759857178\n",
      "6000/11838 Training...batch_loss: 3.3111748695373535\n",
      "7000/11838 Training...batch_loss: 3.386369466781616\n",
      "8000/11838 Training...batch_loss: 3.39422345161438\n",
      "9000/11838 Training...batch_loss: 3.4278712272644043\n",
      "10000/11838 Training...batch_loss: 3.459124803543091\n",
      "11000/11838 Training...batch_loss: 3.420619487762451\n",
      "Epoch: 9/100 loss: 3.37235689163208\n",
      "1000/11838 Training...batch_loss: 3.4930195808410645\n",
      "2000/11838 Training...batch_loss: 3.0876002311706543\n",
      "3000/11838 Training...batch_loss: 3.1744227409362793\n",
      "4000/11838 Training...batch_loss: 3.3360817432403564\n",
      "5000/11838 Training...batch_loss: 3.1967520713806152\n",
      "6000/11838 Training...batch_loss: 3.2956252098083496\n",
      "7000/11838 Training...batch_loss: 3.3708417415618896\n",
      "8000/11838 Training...batch_loss: 3.3792803287506104\n",
      "9000/11838 Training...batch_loss: 3.4132471084594727\n",
      "10000/11838 Training...batch_loss: 3.4445886611938477\n",
      "11000/11838 Training...batch_loss: 3.4062530994415283\n",
      "Epoch: 10/100 loss: 3.3584604263305664\n",
      "save model epoch: 10\n",
      "1000/11838 Training...batch_loss: 3.481574773788452\n",
      "2000/11838 Training...batch_loss: 3.0780982971191406\n",
      "3000/11838 Training...batch_loss: 3.1653101444244385\n",
      "4000/11838 Training...batch_loss: 3.326080560684204\n",
      "5000/11838 Training...batch_loss: 3.186964511871338\n",
      "6000/11838 Training...batch_loss: 3.2850594520568848\n",
      "7000/11838 Training...batch_loss: 3.3602774143218994\n",
      "8000/11838 Training...batch_loss: 3.369127035140991\n",
      "9000/11838 Training...batch_loss: 3.4033257961273193\n",
      "10000/11838 Training...batch_loss: 3.4347259998321533\n",
      "11000/11838 Training...batch_loss: 3.396641492843628\n",
      "Epoch: 11/100 loss: 3.3490841388702393\n",
      "1000/11838 Training...batch_loss: 3.474590301513672\n",
      "2000/11838 Training...batch_loss: 3.071552276611328\n",
      "3000/11838 Training...batch_loss: 3.158466100692749\n",
      "4000/11838 Training...batch_loss: 3.318669080734253\n",
      "5000/11838 Training...batch_loss: 3.1796412467956543\n"
     ]
    }
   ],
   "source": [
    "for epochs in range(train_epochs):\n",
    "    avg_loss = 0\n",
    "    batch_num = 1\n",
    "    for LR, HR, BI in train_loader:\n",
    "        recon = model(LR.to(device))\n",
    "        recon = recon + BI.to(device)\n",
    "        loss = loss_fnc(recon, HR.to(device))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        avg_loss += (loss / batch_size)\n",
    "        \n",
    "        if batch_num % 1000 == 0:\n",
    "            print('{}/{} Training...batch_loss: {}'.format(batch_num, total_batch_num, avg_loss/batch_num))\n",
    "            \n",
    "        batch_num += 1\n",
    "        \n",
    "    avg_loss = avg_loss / total_batch_num\n",
    "    print('Epoch: {}/{} loss: {}'.format(epochs, train_epochs, avg_loss))\n",
    "    \n",
    "    if epochs % 10 == 0:\n",
    "        save_model_path = 'out model/DCSCN_V2_e{}_lr{}_loss{:4}.pt'.format(epochs,lr,loss)\n",
    "        torch.save(model, save_model_path)\n",
    "        print('save model epoch: {}'.format(epochs))\n",
    "\n",
    "save_model_path = 'out model/DCSCN_V2_e{}_lr{}.pt'.format(epochs,lr)\n",
    "torch.save(model, save_model_path)\n",
    "print('final save model epoch: {}'.format(epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35528d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
